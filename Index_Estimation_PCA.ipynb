{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required to estimate the index returns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spd_matrix as spd\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "\n",
    "# Read in the training dataset provided by Optiver\n",
    "all_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is an attempt to estimate the index time series by performing PCA on the stock data\n",
    "## It used the weekly index to estimate the covariance matrix - this ensures we remove market noise\n",
    "## The factor loadings then seem to work as expected and can be thought of as index weights per stock\n",
    "## Clearly the assumption here is that the index weights remain stable throughout the dataset\n",
    "## This assumption is not true, so these weights can be thought of as average index weights over the period\n",
    "\n",
    "# For each week, calculate the stock returns and estimate the covariance matrix - use this covariance matrix in the PCA \n",
    "# to then estimate factor loadings of the first factor (likely the market factor) to each of the stocks\n",
    "end_of_day_data = all_data[all_data['seconds_in_bucket'] == 540]\n",
    "end_of_week_data = end_of_day_data[end_of_day_data['date_id'] % 5 == 0]\n",
    "weekly_ts_prices = end_of_week_data.pivot(index='date_id', columns='stock_id', values = 'wap')\n",
    "weekly_ts_returns = weekly_ts_prices.pct_change(fill_method=None)\n",
    "weekly_ts_returns = weekly_ts_returns.iloc[1:]\n",
    "\n",
    "# Once we have the returns we need to z-score the returns per stock to ensure the PCA does not \"reward\" more volatile ones\n",
    "weekly_ts_returns_zcore = (weekly_ts_returns - weekly_ts_returns.mean()) / weekly_ts_returns.std(ddof=0)\n",
    "\n",
    "# Some of the stocks have missing data for some dates/weeks - we will use the pd.cov() function\n",
    "# This function ignores the missing data and estimates the pairwise correlation which is helpful\n",
    "cov_matrix = weekly_ts_returns.cov()\n",
    "\n",
    "# Given the cov() function estimates pairwise correlation ignoring missing data, it might not be PD\n",
    "# Use the function below to get the nearest PD matrix to the original one before any PCA is done\n",
    "cov_matrix_spd = spd.nearestPD(cov_matrix)\n",
    "\n",
    "# Perform the PCA on this correlation matrix to get the factor loadings of the first factor\n",
    "# This first factor should be the market factor and then we can normalise loadings to get index weights\n",
    "num_assets = len(weekly_ts_returns.columns)\n",
    "pca = PCA(n_components=num_assets, svd_solver='full')\n",
    "pca.fit(cov_matrix_spd)\n",
    "factor_loadings_cov = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "factor_loadings_mkt_cov = factor_loadings_cov[:, 0]\n",
    "factor_loadings_mkt_cov = factor_loadings_mkt_cov / sum(factor_loadings_mkt_cov)\n",
    "\n",
    "idx_wgts = pd.DataFrame(data={'stock_id': list(range(0, num_assets)), 'idx_wgts': factor_loadings_mkt_cov})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to first merge the index weights and replace the index weights for any missing returns data with NaN\n",
    "\n",
    "# Before we do any calculations, let us remove any unnecessary columns to speed up things\n",
    "trim_data = all_data.drop(columns=['imbalance_size', 'imbalance_buy_sell_flag', 'matched_size', 'far_price', 'near_price', 'bid_size', 'ask_size', 'target', 'time_id', 'row_id'])\n",
    "\n",
    "# First, create the return series for each stock as per its respective mid, reference and wap prices\n",
    "trim_data_with_rets = (\n",
    "    trim_data\n",
    "    .assign(\n",
    "        mid_price = lambda x: 0.5 * (x['bid_price'] + x['ask_price']),\n",
    "        ref_return = lambda x: x.groupby(['date_id', 'stock_id'])['reference_price'].pct_change(fill_method=None),\n",
    "        wap_return = lambda x: x.groupby(['date_id', 'stock_id'])['wap'].pct_change(fill_method=None),\n",
    "        mid_return = lambda x: x.groupby(['date_id', 'stock_id'])['mid_price'].pct_change(fill_method=None)\n",
    ")\n",
    ")\n",
    "\n",
    "stock_rets = trim_data_with_rets.drop(columns=['reference_price', 'bid_price', 'ask_price', 'wap', 'mid_price'])\n",
    "stock_rets.to_parquet('stock_rets.gzip', compression='gzip')\n",
    "\n",
    "# Next, let us merge the dataframe with all the data and the one with the index weights\n",
    "stock_rets_idx_wgts = pd.merge(left=stock_rets, right=idx_wgts, on='stock_id')\n",
    "\n",
    "# Now replace any index weights where we do not have returns data with NaN (we will use this later to normalise weights)\n",
    "stock_rets_idx_wgts.loc[stock_rets_idx_wgts['ref_return'].isna(), 'idx_wgts'] = np.nan\n",
    "stock_rets_idx_wgts.loc[stock_rets_idx_wgts['wap_return'].isna(), 'idx_wgts'] = np.nan\n",
    "stock_rets_idx_wgts.loc[stock_rets_idx_wgts['mid_return'].isna(), 'idx_wgts'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have the index weights, we need to normalise them and then estimate the index returns for reference, mid and wap prices\n",
    "\n",
    "# This function creates a normalised version fo the weights for any missing returns data\n",
    "def normalise_wgts(df_sub):\n",
    "    df_sub['norm_wgts'] = df_sub['idx_wgts'] / float(df_sub['idx_wgts'].sum())\n",
    "    return df_sub\n",
    "\n",
    "# Apply the normalise wgts function to our dataframe and remove any unnecessary columns\n",
    "stock_rets_norm_wgts = stock_rets_idx_wgts.groupby(['date_id', 'seconds_in_bucket']).apply(normalise_wgts, include_groups=False)\n",
    "stock_rets_norm_wgts.reset_index(inplace=True)\n",
    "stock_rets_norm_wgts.drop(columns=['level_2', 'idx_wgts'], inplace=True)\n",
    "\n",
    "# Now we can finally estimate the index returns based on the reference price\n",
    "all_idx_rets = (\n",
    "    stock_rets_norm_wgts\n",
    "    .assign(\n",
    "        idx_ref_return = lambda x: x['ref_return'] * x['norm_wgts'],\n",
    "        idx_wap_return = lambda x: x['wap_return'] * x['norm_wgts'],\n",
    "        idx_mid_return = lambda x: x['mid_return'] * x['norm_wgts']\n",
    "    ).groupby(['date_id', 'seconds_in_bucket'], as_index=False)\n",
    "    .agg({'idx_ref_return': 'sum', 'idx_wap_return': 'sum', 'idx_mid_return': 'sum'})\n",
    ")\n",
    "\n",
    "all_idx_rets.to_parquet('all_idx_rets.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The cells below this are ones which were used to test other ways of estimating the index weights\n",
    "## They did not work as well as the covariance approach using weekly stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is an attempt to estimate the index time series by performing PCA on the stock data\n",
    "## It used the weekly index to estimate the correlation matrix - this ensures we remove market noise\n",
    "## The factor loadings then seem to work as expected and can be thought of as index weights per stock\n",
    "## Clearly the assumption here is that the index weights remain stable throughout the dataset\n",
    "## This assumption is not true, so these weights can be thought of as average index weights over the period\n",
    "\n",
    "# For each week, calculate the stock returns and estimate the correlation matrix - use this correlation matrix in the PCA \n",
    "# to then estimate factor loadings of the first factor (likely the market factor) to each of the stocks\n",
    "end_of_day_data = all_data[all_data['seconds_in_bucket'] == max(all_data['seconds_in_bucket'])]\n",
    "end_of_week_data = end_of_day_data[end_of_day_data['date_id'] % 5 == 0]\n",
    "weekly_ts_prices = end_of_week_data.pivot(index='date_id', columns='stock_id', values = 'wap')\n",
    "weekly_ts_returns = weekly_ts_prices.pct_change(1)\n",
    "weekly_ts_returns = weekly_ts_returns.iloc[1:]\n",
    "\n",
    "# Some of the stocks have missing data for some dates/weeks - we will use the pd.corr() function\n",
    "# This function ignores the missing data and estimates the pairwise correlation which is helpful\n",
    "corr_matrix = weekly_ts_returns.corr()\n",
    "\n",
    "# Given the corr() function estimates pairwise correlation ignoring missing data, it might not be PD\n",
    "# Use the function below to get the nearest PD matrix to the original one before any PCA is done\n",
    "corr_matrix_spd = spd.nearestPD(corr_matrix)\n",
    "\n",
    "# Perform the PCA on this correlation matrix to get the factor loadings of the first factor\n",
    "# This first factor should be the market factor and then we can normalise loadings to get index weights\n",
    "num_assets = len(weekly_ts_returns.columns)\n",
    "pca = PCA(n_components=num_assets, svd_solver='full')\n",
    "pca.fit(corr_matrix_spd)\n",
    "factor_loadings_corr = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "factor_loadings_mkt_corr = factor_loadings_corr[:, 0]\n",
    "factor_loadings_mkt_corr = factor_loadings_mkt_corr / sum(factor_loadings_mkt_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This was an attempt to estimate the index time series by performing PCA on the stock data\n",
    "## It used the 10s tick data to estimate the correlation matrix - this tick data will have a lot of noise\n",
    "## The factor loadings did not work as expected due to this noise\n",
    "\n",
    "# For each date, calculate the stock returns and estimate the correlation matrix - use this correlation matrix in the PCA \n",
    "# to then estimate factor loadings of the first factor (likely the market factor) to each of the stocks\n",
    "max_dates = max(all_data['date_id']) + 1\n",
    "max_assets = max(all_data['stock_id']) + 1\n",
    "daily_loadings_mkt = np.empty((max_dates, max_assets,))\n",
    "daily_loadings_mkt[:] = np.nan\n",
    "\n",
    "for date_id in range(max(all_data['date_id']) + 1):\n",
    "    date_data = all_data[all_data['date_id'] == date_id]\n",
    "    # Pivot the dataframe to create a wap time series for all the stocks\n",
    "    date_ts_prices = date_data.pivot(index='time_id', columns='stock_id', values = 'wap')\n",
    "    # Check for any empty columns and remove them\n",
    "    date_ts_prices.dropna(how='all', axis=1, inplace=True)   \n",
    "    stock_ids = sorted(date_ts_prices.columns.unique())\n",
    "    num_assets = len(date_ts_prices.columns)\n",
    "    date_ts_returns = date_ts_prices.pct_change(1)\n",
    "    date_ts_returns = date_ts_returns.iloc[1:]\n",
    "    corr_matrix = date_ts_returns.corr()\n",
    "    pca = PCA(n_components=num_assets, svd_solver='full')\n",
    "    pca.fit(corr_matrix)\n",
    "    factor_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    factor_loadings_mkt = factor_loadings[:, 0]\n",
    "    factor_loadings_mkt = factor_loadings_mkt / sum(factor_loadings_mkt)\n",
    "    daily_loadings_mkt[date_id, stock_ids] = factor_loadings_mkt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
